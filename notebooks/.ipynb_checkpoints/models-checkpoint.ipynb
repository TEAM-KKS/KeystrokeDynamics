{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models:\n",
    "    \"\"\"\n",
    "        Class to handle model related building and execution\n",
    "    \"\"\"\n",
    "    def __init__(self, df : pd.DataFrame, test_size : float=0.2, balance : bool=False):\n",
    "        \"\"\" Output label **MUST ALWAYS** be first column of dataframe \"\"\"\n",
    "        self.df = df\n",
    "        self.test_size = test_size\n",
    "        self.random_state = 111\n",
    "        \n",
    "        self.encode_to_labels([col for col in self.df.columns if df[col].dtype == 'object'])\n",
    "        if balance:\n",
    "            self.split_stratify(self.df.columns[0])\n",
    "        else:\n",
    "            self.split_stratify()\n",
    "        \n",
    "        \n",
    "    def split_stratify(self, balance_by=None):\n",
    "        \"\"\"\n",
    "            Train test split stratified by output label.\n",
    "            To have same samples per class, use balance_by\n",
    "        \"\"\"\n",
    "#         print(\"** Unbalanced stratified per class train_test split via '#text-area-test'\")\n",
    "#         train = self.df[self.df['box_id'] != 3]\n",
    "#         test = self.df[self.df['box_id'] == 3]\n",
    "#         print(train.shape, test.shape)\n",
    "#         self.X_train, self.X_test, self.y_train, self.y_test = train.iloc[:,1:], test.iloc[:,1:], train.iloc[:,0], test.iloc[:,0]\n",
    "#         print(self.X_train.shape, self.X_test.shape, self.y_train.shape, self.y_test.shape)\n",
    "        if balance_by:\n",
    "            grouped = self.df.groupby(balance_by)\n",
    "            balance_df = grouped.apply(lambda x : x.sample(grouped.size().min())).reset_index(drop=True)\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(balance_df.iloc[:,1:], balance_df.iloc[:,0], test_size=self.test_size, stratify=balance_df.iloc[:,0] ,random_state=self.random_state)\n",
    "            print(\"** Balanced stratified per class train_test split\")\n",
    "            print(self.X_train.shape, self.X_test.shape, self.y_train.shape, self.y_test.shape)\n",
    "        else:\n",
    "            print(\"** Unbalanced stratified per class train_test split\")\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.df.iloc[:,1:], self.df.iloc[:,0], test_size=self.test_size, stratify=self.df.iloc[:,0], random_state=self.random_state)\n",
    "            print(self.X_train.shape, self.X_test.shape, self.y_train.shape, self.y_test.shape)\n",
    "            \n",
    "        \n",
    "    def encode_to_labels(self, feature_list):\n",
    "        \"\"\" Encodes all features provided in input list format.\n",
    "            Returns back encoded dataframe and map of of feature -> {label : encoded_class}\n",
    "        \"\"\"\n",
    "        self.feature_encoded = {}\n",
    "        self.labelEncoder = preprocessing.LabelEncoder()\n",
    "        for feature in feature_list:\n",
    "            self.df[feature] = self.labelEncoder.fit_transform(self.df[feature])\n",
    "            self.feature_encoded[feature] = {self.labelEncoder.classes_[i] : i for i in range(len(self.labelEncoder.classes_))}\n",
    "#         return df, feature_encoded\n",
    "        print(\"** Non-numeric columns encoded\")\n",
    "    \n",
    "    def accuracy_score(self, y_pred):\n",
    "        return metrics.accuracy_score(self.y_test, y_pred).round(2)\n",
    "    \n",
    "    def confusion_matrix(self, y_pred):\n",
    "        return metrics.confusion_matrix(self.y_test, y_pred)\n",
    "        \n",
    "    def NB(self):\n",
    "        #Create a Gaussian Classifier\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(self.X_train, self.y_train)\n",
    "        y_pred = gnb.predict(self.X_test)\n",
    "        \n",
    "        return gnb, y_pred\n",
    "    \n",
    "    def LR(self):\n",
    "        logReg = LogisticRegression()\n",
    "        logReg.fit(self.X_train, self.y_train)\n",
    "        y_pred = logReg.predict(self.X_test)\n",
    "#         print(logReg.score(self.X_test, self.y_test))\n",
    "        return logReg, y_pred\n",
    "\n",
    "    def RF(self):\n",
    "        RSEED = 50\n",
    "        rf = RandomForestClassifier(n_estimators=100, \n",
    "                                       random_state=RSEED, \n",
    "                                       max_features = 'sqrt',\n",
    "                                       n_jobs=-1, verbose = 1)\n",
    "\n",
    "        rf.fit(self.X_train, self.y_train)\n",
    "        y_pred = rf.predict(self.X_test)\n",
    "        \n",
    "        n_nodes = []\n",
    "        max_depths = []\n",
    "\n",
    "        # Stats about the trees in random forest\n",
    "        for ind_tree in rf.estimators_:\n",
    "            n_nodes.append(ind_tree.tree_.node_count)\n",
    "            max_depths.append(ind_tree.tree_.max_depth)\n",
    "\n",
    "        print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "        print(f'Average maximum depth {int(np.mean(max_depths))}')\n",
    "        \n",
    "        feature_imp = pd.Series(rf.feature_importances_, index=self.X_train.columns).sort_values(ascending=False)\n",
    "        print(feature_imp)\n",
    "        return rf, y_pred\n",
    "    \n",
    "    def class_score(self, y_pred, model_name):\n",
    "        \"\"\"\n",
    "            Get truth score for each class prediction\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        columns = ['user_id','name','model_name','test_count','predicted','truth']\n",
    "        confusion_matrix = self.confusion_matrix(y_pred)\n",
    "        confusion_matrix_sums = confusion_matrix.sum(axis=1)\n",
    "        labels = {v:k for k,v in self.feature_encoded['name'].items()}\n",
    "        for i in range(len(self.y_test.unique())):\n",
    "            predicited = confusion_matrix[i][i]\n",
    "            truth = 100*(predicited/confusion_matrix_sums[i]).round(2)\n",
    "            rows.append([i, labels[i], model_name, confusion_matrix_sums[i], predicited, truth])\n",
    "            \n",
    "        return pd.DataFrame(rows,columns=columns)\n",
    "        \n",
    "\n",
    "    def classification_report(self, y_pred):\n",
    "        return classification_report(self.y_test, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
