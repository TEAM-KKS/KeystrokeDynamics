{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import itertools\n",
    "import os, glob, re\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_summary(df):\n",
    "    user_counts = df.user_id.value_counts()\n",
    "    num_users = len(df.name.unique())\n",
    "    avg_rows_per_user = np.mean(user_counts.values).round(2)\n",
    "    false_rows = df[df['false_character'] == 't']\n",
    "    high_long_pressed = df[df['long_pressed_equivalent'] > 1]\n",
    "    print(\"******** Data at a high level ********\")\n",
    "    print(\"Number of users              : \", num_users)\n",
    "    print(\"Unique user_ids              : \", len(user_counts))\n",
    "    print(\"Avg rows collected/user_id   : \", avg_rows_per_user)\n",
    "    print(\"Rows with false characters   : {}%\".format(round((false_rows.shape[0]/df.shape[0])*100,2)))\n",
    "    print(\"Avg long_pressed_equivalent  : \", np.mean(df['long_pressed_equivalent']).round(2))\n",
    "    print(\"Rows with long_pressed > 1   : {} %\".format(round((high_long_pressed.shape[0]/df.shape[0])*100,2)))\n",
    "    \n",
    "    print(\"*********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(path, pattern):\n",
    "    \"\"\" Returns all file paths that match a given pattern \"\"\"\n",
    "    return [file for file in glob.glob(path) if re.search(pattern, os.path.basename(file))]\n",
    "\n",
    "def convert_to_pandas(files):\n",
    "    \"\"\" Reads all files into pandas and returns a single merged DataFrame \"\"\"\n",
    "    dfs = [pd.read_csv(file) for file in files]\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "def get_valid_dataset(path_pattern, invalid_platform, user_count_threshold=600):\n",
    "    \"\"\"\n",
    "        1. Parse files in directories with specific patterns, get a working dataset.\n",
    "        2. Remove invalid users based on platform used\n",
    "        3. Remove users for whom total typed characters collected is less than threshold.\n",
    "    \"\"\"\n",
    "    desired_files = [find_files(path, path_pattern[path]) for path in path_pattern]\n",
    "    desired_files = list(itertools.chain(*desired_files))\n",
    "    df = convert_to_pandas(desired_files)\n",
    "    df = df[df['long_pressed_equivalent'] == 1]\n",
    "    \n",
    "    cond1 = ~df['platform'].isin(invalid_platform)\n",
    "    user_typed_count = df.groupby('name').agg({'long_pressed_equivalent':np.sum}).reset_index()\n",
    "    valid_users = user_typed_count[user_typed_count['long_pressed_equivalent'] > user_count_threshold]['name']\n",
    "    cond2 = df['name'].isin(valid_users)\n",
    "    df = df[(cond1) & (cond2)]\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_users_subset(df):\n",
    "    \"\"\"\n",
    "        Given a dataset with multiple user_id(s) per user/name, retrieve only one instance for a user.\n",
    "        Return the instance with highest number of rows collected.\n",
    "    \"\"\"\n",
    "    name_to_user_id = get_num_instances_per_user(df)\n",
    "    users = name_to_user_id[name_to_user_id['user_id'] > 1]['name']\n",
    "    print(f\"*** {len(users)} users with multiple user_ids, taking most significant instance for each user\")\n",
    "    discard = []\n",
    "    for user in users:\n",
    "        discard.extend(df[df['name'] == user].groupby('user_id').agg({'long_pressed_equivalent':np.sum}).sort_values('long_pressed_equivalent').index[:-1].tolist())\n",
    "    \n",
    "    return df[~df['user_id'].isin(list(discard))]\n",
    "\n",
    "def encode_to_labels(df, feature_list) -> (pd.DataFrame, dict):\n",
    "    \"\"\" Encodes all features provided in input list format.\n",
    "        Returns back encoded dataframe and map of of feature -> {label : encoded_class}\n",
    "    \"\"\"\n",
    "    feature_encoded = {}\n",
    "    labelEncoder = preprocessing.LabelEncoder()\n",
    "    for feature in feature_list:\n",
    "        df[feature] = labelEncoder.fit_transform(df[feature])\n",
    "        feature_encoded[feature] = {labelEncoder.classes_[i] : i for i in range(len(labelEncoder.classes_))}\n",
    "    return df, feature_encoded\n",
    "\n",
    "def get_num_instances_per_user(df):\n",
    "    return df.groupby('name').agg({'user_id':'nunique'}).reset_index()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(raw_df, conditions):\n",
    "    \"\"\"\n",
    "        Takes raw data and applies conditions. \n",
    "        Displays some information on the \"prepared\" data.\n",
    "    \"\"\"\n",
    "    df = raw_df[conditions]\n",
    "    char_freq = Counter(df['key_pressed'])\n",
    "    char_df = pd.DataFrame.from_dict(char_freq, orient='index').reset_index()\n",
    "    char_df.columns = ['char','freq']\n",
    "    char_df['type'] = char_df['char'].apply(lambda x : 'lower' if x.islower() else 'upper' if len(x) == 1 and x.isupper() else 'other' )\n",
    "    char_distribution = pd.DataFrame((char_df['type'].value_counts(normalize=True)*100).round(3))\n",
    "    char_type = dict(zip(char_df['char'], char_df['type']))\n",
    "    \n",
    "    df['type'] = df['key_pressed'].map(char_type)\n",
    "    false_char_distribution = pd.DataFrame((df['false_character'].value_counts(normalize=True)*100).round(3))\n",
    "    \n",
    "    print(\"********* DATA PREPARATION FOR FEATURE ENGINEERING *********\")\n",
    "    print(\"\\tRAW VS PREPARED SHAPES\\n{}\\t\\t{}\\n\".format(raw_df.shape, df.shape))\n",
    "    print(\"----------\")\n",
    "    print(\"\\tCHARACTER TYPE DISTRIBUTION\\n\", char_distribution)\n",
    "    print(\"----------\")\n",
    "    print(\"\\tFALSE CHARACTER DISTRIBUTION\\n\", false_char_distribution)\n",
    "    print(\"*************************************************************\")\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def combine_characters_v1(zipped_rows : list, columns : list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Feature engineering support on zipped rows.\n",
    "        `false_character` : Joins all the false_character readings into one\n",
    "        `hold_for` : Sum up\n",
    "        `key_pressed` : Joins all characters pressed\n",
    "        `long_pressed_equivalent` : Sum up\n",
    "        `type_combination` : Comprised of joining the character types (lower, upper, other)\n",
    "        `effort` : Total time spent typing the sequence of N characters.\n",
    "        `speed` : How efficiently and quickly was the sequence typed. \n",
    "    \"\"\"\n",
    "    n = len(zipped_rows[0])\n",
    "    feature_rows = []\n",
    "    skipped = 0\n",
    "    for row in tqdm.tqdm(zipped_rows):\n",
    "        row = pd.DataFrame(row, columns=columns)\n",
    "        if len(row['box_id'].unique()) == 1 and len(row['name'].unique()) == 1:\n",
    "\n",
    "            result = row.loc[0]\n",
    "            result['false_character'] = ''.join(row['false_character'])\n",
    "            result['hold_for'] = sum(row['hold_for'])\n",
    "            result['key_pressed'] = ''.join(row['key_pressed'])\n",
    "            result['long_pressed_equivalent'] = sum(row['long_pressed_equivalent'])\n",
    "            result['pressed_after'] = sum(row['pressed_after'])\n",
    "            result['type_combination'] = ''.join(['l' if x.islower() else 'u' if x.isupper() else 'o' for x in row['key_pressed']])\n",
    "            result['effort'] = sum(row['hold_for']) + sum(row['pressed_after'][1:])\n",
    "            result['speed'] = np.mean([(abs(ord(x[0].lower()) - ord(x[1].lower())) / row['pressed_after'][i+1]) for i,x in enumerate(zip(row['key_pressed'][:-1], row['key_pressed'][1:]))]).round(4)\n",
    "            if result['speed'] == float('inf'):\n",
    "#                 print('Skipping row as `speed` evaluates to `inf`... ', row['id'])\n",
    "                skipped += 1\n",
    "                continue\n",
    "                    \n",
    "            feature_rows.append(result)\n",
    "    print(\"****** Skipped {} rows as `speed` evaluates to `inf`\".format(skipped))\n",
    "    return pd.DataFrame(feature_rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_summary(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        At a high level, derive the characteristics of a user split by `TRUE_CHAR` and `FALSE_CHAR`.\n",
    "        Total characters captured, time spent and respective percentages.\n",
    "    \"\"\"\n",
    "    user_summary = []    \n",
    "    columns = ['name','device_type','platform','type','total_characters','total_time_spent','character_perc','character_time_perc']\n",
    "\n",
    "    for user in tqdm.tqdm(df.name.unique()):\n",
    "        user_row = [user]\n",
    "        u_df = df[df['name'] == user]\n",
    "        \n",
    "        user_row = [user, u_df['device_type'].unique()[0], u_df['platform'].unique()[0]]\n",
    "        \n",
    "        true_df = u_df[u_df['false_character'] == 'f']\n",
    "        false_df = u_df[u_df['false_character'] == 't']\n",
    "\n",
    "        chars_captured = u_df['long_pressed_equivalent'].sum()\n",
    "        true_chars_captured = true_df['long_pressed_equivalent'].sum()\n",
    "        false_chars_captured = false_df['long_pressed_equivalent'].sum()\n",
    "\n",
    "        time_spent = np.sum(u_df['hold_for'] + u_df['pressed_after'])\n",
    "        time_spent_true = np.sum(true_df['hold_for'] + true_df['pressed_after'])\n",
    "        time_spent_false = np.sum(false_df['hold_for'] + false_df['pressed_after'])\n",
    "        \n",
    "        row = user_row.copy()\n",
    "        row.extend(['TRUE_CHAR',true_chars_captured, time_spent_true,\n",
    "                   (true_chars_captured/chars_captured)*100,\n",
    "                   (time_spent_true/time_spent)*100])\n",
    "        \n",
    "        user_summary.append(row)\n",
    "        row = user_row.copy()\n",
    "        row.extend(['FALSE_CHAR',false_chars_captured, time_spent_false,\n",
    "                   (false_chars_captured/chars_captured)*100,\n",
    "                   (time_spent_false/time_spent)*100])\n",
    "        user_summary.append(row)\n",
    "    \n",
    "    return pd.DataFrame(user_summary, columns=columns)\n",
    "\n",
    "def clean_raw_data(raw : pd.DataFrame, true_perc_threshold=60) -> pd.DataFrame:\n",
    "    raw = raw[raw['long_pressed_equivalent'] == 1]\n",
    "    user_agg_test = user_summary(raw)\n",
    "    \n",
    "    invalid_users = user_agg_test[(user_agg_test['type'] == 'TRUE_CHAR') & (user_agg_test['character_perc'] < true_perc_threshold)]\n",
    "    valid_users = user_agg_test[(user_agg_test['type'] == 'TRUE_CHAR') & (user_agg_test['character_perc'] >= true_perc_threshold)]\n",
    "    print(\"******** TRUE_CHAR_MEAN ********\")\n",
    "    print(f\"Removing users with avg character perc for TRUE less than {true_perc_threshold} (TRUE PERC THRESHOLD)\")\n",
    "    print(f\"ALL USERS AVG    : \", user_agg_test[(user_agg_test['type'] == 'TRUE_CHAR')]['character_perc'].mean())\n",
    "    print(\"INVALID USERS AVG : \", invalid_users['character_perc'].mean())\n",
    "    print(\"VALID USERS AVG   : \", valid_users['character_perc'].mean())\n",
    "    print(\"********************************\")\n",
    "    raw_clean = raw[raw['name'].isin(valid_users['name'].unique())]\n",
    "    unique_users = len(raw_clean['name'].unique())\n",
    "    print(f\"{unique_users} valid users found, data is cleaned for exploration and feature engineering!\")\n",
    "    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "    return raw_clean\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
